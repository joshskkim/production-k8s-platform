# .github/workflows/terraform.yml
name: Terraform with Auto-Cleanup
on:
  push:
    branches: [main]
    paths: ['terraform/**']
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
        - deploy
        - cleanup-only
      environment:
        description: 'Environment to target'
        required: true
        default: 'development'
        type: choice
        options:
        - development
        - staging
        - production
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

env:
  AWS_REGION: us-east-1

jobs:
  terraform-deploy:
    if: ${{ github.event.inputs.action != 'cleanup-only' && github.event.schedule == null }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [development]
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.12.0'

      - name: Terraform Deploy
        run: |
          cd terraform/environments/${{ matrix.environment }}
          terraform init
          terraform validate
          terraform plan -out=tfplan
          terraform apply tfplan

      - name: Deploy Monitoring
        run: |
          cd terraform/environments/${{ matrix.environment }}
          sleep 60
          if [ -f "./deploy-monitoring.sh" ]; then
            chmod +x ./deploy-monitoring.sh
            ./deploy-monitoring.sh
          fi

      - name: Verify Deployment
        run: |
          CLUSTER_NAME="${{ matrix.environment }}-payment-platform-cluster"
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name $CLUSTER_NAME
          kubectl get nodes
          kubectl get pods -n monitoring --ignore-not-found=true

      # AUTO-DESTROY for development after 30 minutes
      - name: Wait and Auto-Cleanup (Dev Only)
        if: matrix.environment == 'development'
        run: |
          echo "‚è∞ Waiting 30 minutes before auto-cleanup..."
          sleep 1800  # 30 minutes
          
          echo "üßπ Starting auto-cleanup for development environment..."
          
          # Cleanup monitoring first
          CLUSTER_NAME="${{ matrix.environment }}-payment-platform-cluster"
          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} &>/dev/null; then
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name $CLUSTER_NAME
            
            # Remove monitoring stack
            helm uninstall promtail -n monitoring --ignore-not-found || true
            helm uninstall loki -n monitoring --ignore-not-found || true
            helm uninstall kube-prometheus-stack -n monitoring --ignore-not-found || true
            helm uninstall prometheus-operator-crds -n monitoring --ignore-not-found || true
            kubectl delete namespace monitoring --ignore-not-found=true || true
            
            # Wait for cleanup
            sleep 60
          fi
          
          # Destroy infrastructure
          cd terraform/environments/${{ matrix.environment }}
          terraform destroy -auto-approve
          
          echo "‚úÖ Development environment auto-cleanup completed"

  # Manual cleanup job
  terraform-cleanup:
    if: ${{ github.event.inputs.action == 'cleanup-only' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Tools
        uses: hashicorp/setup-terraform@v3

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.12.0'

      - name: Manual Cleanup
        run: |
          ENV="${{ github.event.inputs.environment || 'development' }}"
          CLUSTER_NAME="$ENV-payment-platform-cluster"
          
          echo "üßπ Cleaning up $ENV environment..."
          
          # Cleanup monitoring if cluster exists
          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} &>/dev/null; then
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name $CLUSTER_NAME
            helm uninstall promtail -n monitoring --ignore-not-found || true
            helm uninstall loki -n monitoring --ignore-not-found || true
            helm uninstall kube-prometheus-stack -n monitoring --ignore-not-found || true
            kubectl delete namespace monitoring --ignore-not-found=true || true
            sleep 30
          fi
          
          # Terraform destroy
          cd terraform/environments/$ENV
          terraform init
          terraform destroy -auto-approve

  # Scheduled orphan resource cleanup
  cleanup-orphaned:
    if: github.event.schedule
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Find and Cleanup Orphaned Resources
        run: |
          echo "üîç Scanning for orphaned AWS resources..."
          
          # Find old EKS clusters
          echo "Checking EKS clusters..."
          aws eks list-clusters --region ${{ env.AWS_REGION }} --output table
          
          # Find old load balancers
          echo "Checking load balancers..."
          aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `payment-platform`) || contains(LoadBalancerName, `development`)].{Name:LoadBalancerName,Created:CreatedTime}' --output table
          
          # Find old security groups
          echo "Checking security groups..."
          aws ec2 describe-security-groups --filters "Name=group-name,Values=*payment-platform*" --query 'SecurityGroups[].{Name:GroupName,ID:GroupId}' --output table
          
          # Find old VPCs
          echo "Checking VPCs..."
          aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*payment-platform*" --query 'Vpcs[].{Name:Tags[?Key==`Name`].Value|[0],VpcId:VpcId,State:State}' --output table
          
          # Find old RDS instances
          echo "Checking RDS instances..."
          aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `payment-platform`)].{Name:DBInstanceIdentifier,Status:DBInstanceStatus}' --output table
          
          # Find old ElastiCache clusters
          echo "Checking ElastiCache..."
          aws elasticache describe-cache-clusters --query 'CacheClusters[?contains(CacheClusterId, `payment-platform`)].{Name:CacheClusterId,Status:CacheClusterStatus}' --output table
          
          # Cleanup old development resources (older than 6 hours)
          CUTOFF_TIME=$(date -u -d '6 hours ago' '+%Y-%m-%dT%H:%M:%SZ')
          echo "Cleaning resources created before: $CUTOFF_TIME"
          
          # Delete old development EKS clusters
          CLUSTERS=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --output text --query 'clusters[]')
          for cluster in $CLUSTERS; do
            if [[ $cluster == *"development"* ]]; then
              CREATED=$(aws eks describe-cluster --name $cluster --query 'cluster.createdAt' --output text)
              if [[ "$CREATED" < "$CUTOFF_TIME" ]]; then
                echo "üóëÔ∏è Deleting old development cluster: $cluster"
                # Note: This is aggressive - use with caution
                # aws eks delete-cluster --name $cluster --region ${{ env.AWS_REGION }} || true
                echo "Found old cluster: $cluster (would delete in production cleanup)"
              fi
            fi
          done

      - name: Report Cleanup Status
        run: |
          echo "üìä Cleanup scan completed"
          echo "Check the logs above for any resources that need attention"